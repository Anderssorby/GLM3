---
title: "Logistic regression and Poisson regression"
author: "Anders Christiansen Sørby, Edvard Hove, Angela Maiken Johnsen"
date: "October 4, 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("ggplot2")
#install.packages("GGally")
#install.packages("rlist")
#install.packages("lme4")
#install.packages("sjPlot")
#install.packages("ggpubr")
library(ggplot2)
library(rlist)
library(reshape2)
library(plyr)
library(GGally)
library(sjPlot)
library(ggpubr)

my_dens <- function(data, mapping, ...) {
  ggplot(data = data, mapping=mapping) +
    geom_density(..., alpha = 1, fill = NA) 
}

```


# a)
```{r}
dataset <- read.table("https://www.math.ntnu.no/emner/TMA4315/2018h/jsp2.txt", header = TRUE)
ggpairs(data = dataset, mapping = aes(col = gender, alpha=0.7), columns = c("social", "raven", "math"), legend = 1)
```
From the plot it can be seen from the top and bottom right panel that girls have higher average maths scores than boys. This is because the mean of the blue box plots in the top right panel (corresponding to the girls' maths scores) are higher than that of the boys' (red boxes). However, as 0 is included in all the box plots, the difference is not too big. The same conclusion can be drawn from the bottom right panel; here the distribution of the girls' maths scores have a peak slightly more to the right than that of the boys.  

The middle section shows that the gender differences are small for the raven tests. However mostly there are few differences between the different social class. 

Fitting a linear model with math as response and raven and gender as covariates:
```{r}
model = lm(math ~ raven + gender, data = dataset)
summary(model)
```
The model for the $k$th student is thus

$$
Y_k={\bf x}_k \pmb{\beta} + \varepsilon_k,
$$
where the $\varepsilon_k$s are independent, have mean 0 and variance $\sigma^2$. Here the $Y_k$ is the response, $\bf x_k$ are the covariates, $\varepsilon_k$ are the errors and $\pmb{\beta}$ the fixed effects parameters.
Here the parameter estimates are $\hat\beta_0 = -1.313$,  $\hat\beta_{\mathrm{raven}} = 0.1965$ and $\hat\beta_{\mathrm{gendergirl}} = 2.538$. This means that being a girl has a positive impact on the maths test score, and a positive raven test score also gives a positive impact on the maths test score. Here $\hat\beta_0$, i. e. the intercept, corresponds to the parameter estimate for a boy. 
By looking at the $p$ values of the parameter estimates, all of them are highly significant. 
In this model we are investigating if there is a linear relationship between maths test score and raven test score and gender, and we are assuming that the maths test score does not depend on social status and which school the student comes from.

# b)
Now a random intercept model with math as response and raven and gender as covariates as before, in addition to school as a random intercept. This can be written mathematically as

$$
{\bf Y}_{i} = {\bf X}_i\beta + {\bf 1} \gamma_{0i} + \varepsilon_{i}
$$
As before $\bf Y_i$ is the response, $\bf X_i$ the covariates, $\beta$ the fixed effects (and $\beta_0$ the fixed population intercept), $\gamma_{0i}$ the deviation (for members of cluster $i$) from the population intercept $\beta_0$.
The response, $\bf Y_i$, is a $n_i\times 1$ vector, where $n_i$ is the number of observations from cluster $i$.
Each student has its own row in the matrix $\bf X_i$.
Each row contains an intercept and 2 covariates, making it an $n_i \times 3$ matrix, and $\beta$ a $3 \times 1$ vector.
The vector ${\bf 1}$ is a $n_i\times 1$ vector consisting of only 1's, and realizations of the random variable $\gamma_{0i}$ are scalars.
Lastly $\varepsilon_{i}$ has the same dimensions as $Y_i$, that is $n_i\times 1$.
In our case this means the number of students from school $i$.
We assume that $\gamma_{0i} \sim \mathcal{N}(0,\tau_0^2)$ and $\varepsilon_{i} \sim \mathcal{N}_{n_i}(0,\sigma^2I)$.
It is also assumed that the $\gamma_{0i}$ are independent, and also independent from all the $\varepsilon_{i}$ vectors.
We also assume that the responses at school $i$ and $k$ are independent.

```{r}
library(lme4)
fitRI1 <- lmer(math ~ raven + gender + (1 | school), data = dataset)
summary(fitRI1)
```
The estimates for this model are more or less in agreement with those of the previous one.
Girls and those who do well on the raven test are still expected to do better on the math test.
The model predicts girls to score 2.511 points higher on the test than boys, and a student who scores 1 point higher on the raven test is predicted to score 0.214 points higher on the math test.
The new model estimates the effect from a good raven test to be slightly higher (0.214 vs 0.197), and the effect of gender to be slightly lower (2.511 vs 2.538).

The p-values are omitted from the printout because the common assumptions used to compute p-values does not hold in this model. There are no analytical results for the null distribution of parameter estimates in complex situations for mixed models. The null distribution is the distribution of the test statistic when the null hypothesis is true. In general the parameter estimates are not t-distributed for finite size samples though it holds asymptotically.

We can test the siginificance of the $\beta_{raven}$ parameter by doing a hypothesis test $H_0: \beta_{raven} = 0$ against $H_1:\beta_{raven} \neq 0$. This can be done with a Wald-test which in this case reduces to a Z-test. 
```{r }
pvalue <- 2*(1-pnorm(abs(9.197)))
pvalue
```
The t-value for raven is rather big so the following pvalue is very small. We are forced to reject the null hypothesis. 

Also we can compute a $(1-\alpha)\cdot100\% = 95\%$-confidence interval for the effect of female gender on math score. 
\begin{align*}
\text{CI}_{j} &= \left[\hat\beta_{j} - \Phi^{-1}(1-\alpha/2)\cdot \widehat{SD}(\hat\beta_{j}),\  \hat\beta_{j} + \Phi^{-1}(1-\alpha/2)\cdot \widehat{SD}(\hat\beta_{j}) \right] \\
&= [2.51119 - 1.96\cdot 0.26684 , 2.51119 + 1.96\cdot 0.26684  ] \\
&= [1.988184, 3.034196]
\end{align*}

# c)

We will now consider a model without the gender covariate.

```{r }
fitRI2 <- lmer(math ~ raven + (1 | school), data = dataset)
summary(fitRI2)
```

The intra class covariance between $Y_{ij}$ and $Y_{il}$ is
\begin{equation}
\text{Cov}({\bf Y}_i)=\tau_0^2 \mathbf{11}^T + \sigma^2 \mathbf{I}
\end{equation}
with correlation
\begin{equation}
\text{Corr}(Y_{ij},Y_{il})=\frac{\text{Cov}(Y_{ij},Y_{il})}{\sqrt{\text{Var}(Y_{ij})\text{Var}(Y_{il})}}=\frac{\tau_0^2}{\tau_0^2+\sigma^2} \text{ for }j\neq l.
\end{equation}

From the summary of the model we get $\hat\sigma^2=20.711$ and $\hat\tau_0^2=4.002$ which means that $\text{Corr}(Y_{ij},Y_{il}) = 0.1619$. This correlation is rather low. The variance between different students has far more to say than the variation between schools which makes sense. However there is still a noticable effect of different schools.

In general we can consider a joint distribution of $Y$ and $\gamma \sim N(0,G)$
\begin{equation}
\begin{pmatrix} {\bf Y}\\ {\boldsymbol \gamma} \end{pmatrix} \sim
N(\begin{pmatrix} {\bf X}{\boldsymbol \beta} \\ {\bf 0}\end{pmatrix},
\begin{pmatrix}{\bf V}={\bf U}{\bf G}{\bf U}^T+\sigma^2{\bf I} & {\bf U}{\bf G}\\
{\bf G}{\bf U}^T & {\bf G}\end{pmatrix})
\end{equation}
where $G$ is a block diagonal matrix of $Q$ which is the covariance matrix of $\gamma_i$. $U$ is a design matrix for random effects similar to $X$ for fixed effects.

We can estimate the random intercept as follows
\begin{equation}
\hat{{\boldsymbol \gamma}}_i=\hat{\bf Q}{\bf U}_i^T\hat{\bf V}_i^{-1}({\bf Y}_i-{\bf X}_i\hat{{\boldsymbol \beta}})
\end{equation}
where in our model $U_i=\mathbf 1$ and $\mathbf Q=\tau_0^2$. Then for our model it reduces to
\begin{equation}
\hat{\gamma}_{0i}=\frac{n_i \hat{\tau}_{0}^2}{\hat{\sigma}^2+n_i \hat{\tau}_{0}^2}e_i
\end{equation}
where $e_i=\frac{1}{n_i} \sum_{j=1}^{n_i} (Y_{ij}-{\bf x}_{ij}^T\hat{{\boldsymbol \beta}})$ is the average residual. 

```{r plots-c1}
gg1 <- plot_model(fitRI2, type = "diag", prnt.plot = FALSE, geom.size = 1)
gg2 <- plot_model(fitRI2, type = "re", sort.est = "(Intercept)", y.offset = 0.4, dot.size = 1.5) +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) + 
  labs(title = "Random intercept (RI)", x = "school", y = "math")
gg3 <- ggplot(data = data.frame(x = ranef(fitRI2)$school[[1]]), aes(x = x)) + geom_density() + 
  labs(x = "math", y = "density", title = "Density of RI") +
  stat_function(fun = dnorm, args = list(mean = 0, sd = attr(VarCorr(fitRI2)$school, "stddev")), col = "red")
df <- data.frame(fitted = fitted(fitRI2), resid = residuals(fitRI2, scaled = TRUE))
gg4 <- ggplot(df, aes(fitted,resid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Residuals", title = "Residuals vs Fitted values")
gg5 <- ggplot(df, aes(sample=resid)) + stat_qq(pch = 19) +
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", title = "Normal Q-Q")

gg1[[2]]$school + ggtitle("QQ-plot of random intercepts")
ggarrange(gg2, gg3, gg4, gg5)
```

```{r plots-c2}
df <- data.frame(x = rep(range(dataset$raven), each = 49),
                 y = coef(fitRI2)$school[,1] + coef(fitRI2)$school[,2] * rep(range(dataset$raven), each = 49),
                 School = factor(rep(c(1:42, 44:50), times = 2)))
```

```{r plots-c3, fig.cap="Random intercept realization for each school.\\label{fig:rir}"}
ggplot(df, aes(x = x, y = y, col = School)) + geom_line() + labs(x = "raven score", y = "math score")
```

The QQ-plot of random intercepts is used to check the normal assumption of the $\gamma_i$’s. In this plot the quantiles agree fairly well, as the points more or less correspond to a straight line. Here there is an outlier in the bottom left corner. 

The plot named “Random intercept (RI)” can be used to check whether there is a difference between the Ris or not. 

The plot showing the density of RI can be used to compare the observed density of the math scores to the density of the normal distribution with mean 0 and standard deviation $\hat\tau_0$. They are somewhat in agreement, however there are some outliers in the left tail. But as there are only 49 schools, which is a rather low number of schools, one cannot expect the densities to agree perfectly as one would need more data.
 

The plot named “Residuals vs. Fitted values” is used to check if there is a relationship between the residuals and the fitted values. Ideally the points should scatter heterogeneously around the horizontal dotted line (0). There seems to be a trend for the more extreme fitted values, but for values where we have lots of data we observe the wanted behaviour. The unwanted behaviour at the extreme values may be due to chance which should be investigated further.

The normal QQ plot checks the normality assumption of the standardised residuals. Ideally the points should follow the dashed line. Here the quantiles agree, as the points lie more or less on the dashed line.

The final plot \ref{fig:rir} shows the baseline effect for math score of each school. It shows that there is a distribution between the schools that is affecting the scores and how we model this using a random intercept. 

# d)



```{r model-d}
fitRI3 <- lmer(math ~ raven + social + (1 | school), data = dataset)
anova(fitRI2, fitRI3)
```

We are not using REML estimation for the parameters of when doing the ANOVA test. This is because it would otherwise break the assumption of nested models. <!--Er vi helt sikre på dette?-->|

```{r model-d2}
fitRIS <- lmer(math ~ raven + (1 + raven | school), data = dataset)
summary(fitRIS)
```
Given this ANOVA test we conclude that the model including the social status of the father is slightly better. This is given a significance level $\alpha= 0.05\geq \text{p-value}$. The AIC for this model is also slightly lower than the one without the social covariate. This points in the direction of selecting this model. However the BIC is larger, which is natural because the penalty term for parameters is larger. We choose to rely on the AIC and the likelihood ratio test to select model.

```{r plots-d}
df <- data.frame(x = rep(range(dataset$raven), each = 49),
                 y = coef(fitRIS)$school[,1] + coef(fitRIS)$school[,2] * rep(range(dataset$raven), each = 49),
                 School = factor(rep(c(1:42, 44:50), times = 2)))
gg1 <- ggplot(df, aes(x = x, y = y, col = School)) + geom_line()

gg2 <- plot_model(fitRIS, type = "re", sort.est = "(Intercept)", y.offset = 0.4, dot.size = 1.5) +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) + labs(title = "Random intercept (RI)")

ggarrange(gg1, gg2, ncol = 2, legend = FALSE)
```

The random intercept and slope model we have fitted has formula

\begin{equation}
\hat{\beta}_0+\hat{\beta}_1\text{raven}+\hat{\gamma}_{i0}+\hat{\gamma}_{1i}\text{raven}
\end{equation}

We see from the model that there is a correlation between the random intercept and the random slope

# e)

If we want to model the probability for a student to fail maths we are considering a binary response, sucess or failiure, which means it's a Bernoulli (or ungrouped binomial) distribution. This means we need to use a Generalized Linear Mixed Model (GLMM).
